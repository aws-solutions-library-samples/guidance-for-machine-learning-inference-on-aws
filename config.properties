#!/bin/bash

# This file contains all customizable configuration items for the project
# core version to be used at re:Invent 23 builder sessions

######################################################################
# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. #
# SPDX-License-Identifier: MIT-0                                     #
######################################################################

# Project settings
## verbose - show verbose output, verbose=true(default)|false
## Unless verbose is explicitly set to false, verbose output is enabled
export verbose=true
## dollar - env variable that corresponds to the dollar sign used in variable substitution
export dollar="\$"
## target_platform - target inference platform for the project, target_platrorm=huggingface(default)|nim
export target_platform=nim

# NIM settings
## nim_model_name - name of the model embedded in the nim container
nim_model_name=llama3-8b-instruct
## nim_api_key - NGC CLI API Key allowing access to pull the nim container
nim_api_key="required"
base64_opts=""
base64_is_gnu=$(base64 --version 2>&1 | grep base64 | grep GNU | wc -l | xargs)
if [ "$base64_is_gnu" == "1" ]; then base64_opts="-w 0"; fi
nim_api_key_b64o=$(echo $nim_api_key | base64 ${base64_opts})
nim_api_key_sublen=$(( ${#nim_api_key_b64o} - 2 ))
if [ "$nim_api_key_sublen" -gt 0 ]; then
	nim_api_key_b64="${nim_api_key_b64o:0:$nim_api_key_sublen}=="
else
	nim_api_key_b64=""
fi
nim_registry=nvcr.io/nim/meta/
nim_image_name=${nim_model_name}
nim_image_tag=:1.0.0

# Huggingface settings
huggingface_model_name=bert-base-multilingual-cased
huggingface_tokenizer_class=BertTokenizer
huggingface_model_class=BertForQuestionAnswering

# Compiler settings
## processor = cpu|gpu|inf1|inf2|graviton
processor=gpu
pipeline_cores=1
sequence_length=128
batch_size=1
test_traced_model=True

# Packer settings
## model_server - the name of a supported model server. model_server=fastapi(default)|triton|torchserve|nim
model_server=nim
base_image_triton=nvcr.io/nvidia/tritonserver:24.01-py3
base_image_torchserve=pytorch/torchserve:latest-${processor}

# AWS Settings
## account is the current AWS user account. This setting is determined automatically.
account=$(aws sts get-caller-identity --query Account --output text)
## region is used to login if the registry is ecr 
region=us-west-2

# Container settings
## Default is the private ECR registry in the current AWS account.
## If registry is set, include the registry uri up to the image name, end the registry setting with /
## registry setting for locally built images uploaded to local ECR
registry=${account}.dkr.ecr.${region}.amazonaws.com/
## registry_type=ecr
registry_type=ecr
base_image_name=aws-do-inference-base
base_image_tag=:v15-${processor}
#model_image_name=${huggingface_model_name}
#model_image_tag=:v15-${model_server}-${processor}
model_image_name=${nim_image_name}
model_image_tag=${nim_image_tag}

# if using pre-built public ECR registry Model image (may require authentication) use the following settings for model_image
#registry=public.ecr.aws/a2u7h5w3/
#model_image_name=bert-base-workshop
#model_image_tag=:v15-${processor}

# Trace settings
## trace_opts_$processor is a processor-specific setting used by the docker run command in the trace.sh script
## This setting will be automatically assigned based on your processor value
trace_opts_cpu=""
trace_opts_gpu="--gpus 0"
trace_opts_inf1="-e AWS_NEURON_VISIBLE_DEVICES=ALL --privileged"
trace_opts_inf2="-e AWS_NEURON_VISIBLE_DEVICES=ALL --privileged"
trace_opts_graviton=""

# Deployment settings
## some of these settings apply only when the runtime is kubernetes
## runtime = docker | kubernetes
runtime=kubernetes
## num_servers - number of model servers to deploy
## note that more than one model server can run on a node with multiple cpu/gpu/inferentia chips.
## example: 4 model servers fit on one inf1.6xlarge instance as it has 4 inferentia chips.
num_servers=1
## number of models per model server
num_models=1
## quiet = False | True - sets whether the model server should print logs
quiet=False
## postprocess = True | False - sets whether tensors returned from model should be translated back to text or just returned
postprocess=True
## model_server_port = 8080(default/fastapi)|8000(triton)
model_server_port=8080
if [ "$model_server" == "triton" ]; then model_server_port=8000; fi
## service_port=8080 - port on which model service will be exposed
service_port=8080
# Kubernetes-specific deployment settings
# instance_type = c5.xxx | g4dn.xlarge | g4dn.12xlarge | inf1.xlarge | inf2.8xlarge | c7g.4xlarge...
# A node group with the specified instance_type must exist in the cluster
# The instance type must have the processor configured above
# Example: processor=graviton, instance_type=c7g.4xlarge
instance_type=g5.8xlarge
# Kubernetes namespace
namespace=default
## Kubernetes app name
#app_name=${huggingface_model_name}-${processor}
app_name=${nim_model_name}-${processor}
app_dir=app-${app_name}-${instance_type}

# Test settings
## Test processor - the processor to use for running test client. test_processor=cpu|graviton
test_processor=cpu
## Test image - locally built images
#test_image_name=test-${huggingface_model_name}
test_image_name=test-${nim_model_name}
test_image_tag=:v15-${test_processor}

##when using pre-built test image available in public ECR registry (may require authentication): 
##test_image_name=bert-base-workshop
##test_image_tag=:test-v11-cpu

## request_frequency - time to sleep between two consecutive requests in curl tests
request_frequency=0.01
## Stop random request test after num_requests number of requests
num_requests=30
## Number of test containers to launch (default=1), use > 1 for scale testing
num_test_containers=1
## test_instance_type - when runtime is kubernetes, node instance type on which test pods will run
test_instance_type=c5.2xlarge
## test_namespace - when runtime is kubernetes, namespace where test pods will be created
test_namespace=default
## test_dir - when runtime is kubernetes, directory where test job/pod manifests are stored
test_dir=app-${test_image_name}-${instance_type}
